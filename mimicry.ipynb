{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "These are our imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#imports 2021.2.576440691\n",
    "import numpy as np\n",
    "print(\"numpy imported\")\n",
    "import json\n",
    "print(\"json imported\")\n",
    "import time\n",
    "print(\"time imported\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"tensorflow imported\") \n",
    "from keras import Sequential\n",
    "print(\"keras imported\")\n",
    "import config\n",
    "channelID,serverID,targetID = config.getScope()\n",
    "print(\"config imported\")\n",
    "import pickle\n",
    "print(\"pickle imported\")\n",
    "import random\n",
    "print(\"random imported\")\n",
    "import string\n",
    "print(\"string imported\")\n",
    "from scipy.sparse import csc_matrix\n",
    "print(\"scipy imported\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the section below to process your data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def exists(filename):\r\n",
    "    try:\r\n",
    "        f = open(filename, \"rb\")\r\n",
    "        f.close()\r\n",
    "        return True\r\n",
    "    except:\r\n",
    "        return False\r\n",
    "if exists(\"message.list\") == False:\r\n",
    "    channelID,serverID,targetID = config.getScope()\r\n",
    "    daTestDoc = open(\"TestDoc\",\"r\",encoding=\"utf-8\")\r\n",
    "    daMessageList = eval(daTestDoc.read())\r\n",
    "    with open('message.list','wb') as messageList_file:\r\n",
    "        pickle.dump(daMessageList,messageList_file)\r\n",
    "        print(\":flushed:\")\r\n",
    "print('not flushed')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputList,outputList = [],[]\r\n",
    "with open(\"message.list\",'rb') as daMessageList:\r\n",
    "    channelID,serverID,targetID = config.getScope()\r\n",
    "    daMessageList = pickle.load(daMessageList)\r\n",
    "    for i in range(len(daMessageList)):\r\n",
    "        if i != len(daMessageList)+1:\r\n",
    "            message = daMessageList[i]\r\n",
    "            if message[\"author\"][\"id\"] == str(targetID):\r\n",
    "                #if i != 0:\r\n",
    "                if True:\r\n",
    "                    previousMessage = daMessageList[i-1]\r\n",
    "                    if previousMessage[\"author\"][\"id\"] == str(targetID):\r\n",
    "                        if len(outputList) != 0:    \r\n",
    "                            outputList[len(outputList)-1] += \"   \" + message[\"content\"]\r\n",
    "                    else:\r\n",
    "                        if type(message.get(\"referenced_message\"))!=dict:\r\n",
    "                            outputList.append(message[\"content\"])\r\n",
    "                            message = daMessageList[i+1]\r\n",
    "                            inputList.append(message[\"content\"])\r\n",
    "                        else:\r\n",
    "                            inputList.append(message[\"referenced_message\"][\"content\"])\r\n",
    "                            outputList.append(message[\"content\"])\r\n",
    "\r\n",
    "        myIndice = random.randint(0,len(inputList))\r\n",
    "\r\n",
    "    # for message in range(len(inputList)):\r\n",
    "    # \tinputList[message] = removePunctuation(removeCaps(inputList[message]))\r\n",
    "    with open('input.list','wb') as inputList_file:\r\n",
    "        pickle.dump(inputList,inputList_file)\r\n",
    "    with open('output.list','wb') as outputList_file:\r\n",
    "        pickle.dump(outputList,outputList_file)\r\n",
    "    print(\"Complete! (1/2)\")\r\n",
    "\r\n",
    "\r\n",
    "with open('input.list','rb') as inputList_file:\r\n",
    "    with open('output.list','rb') as outputList_file:\r\n",
    "        outputList = pickle.load(outputList_file)\r\n",
    "        inputList = pickle.load(inputList_file)\r\n",
    "        \r\n",
    "        removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\r\n",
    "        removeCaps = lambda x: x.lower()\r\n",
    "\r\n",
    "        for message in range(len(inputList)):\r\n",
    "            inputList[message] = removePunctuation(removeCaps(inputList[message]))\r\n",
    "            outputList[message] = removePunctuation(removeCaps(outputList[message]))\r\n",
    "            inputList[message] = \"<bos>\" + inputList[message] + \"<eos>\"\r\n",
    "            outputList[message] = \"<bos>\" + outputList[message] + \"<eos>\"\r\n",
    "        #inputList = np.array(inputList)\r\n",
    "        #outputList = np.array(outputList)\r\n",
    "        with open('input.list','wb') as inputList_file:\r\n",
    "            pickle.dump(inputList,inputList_file)\r\n",
    "        with open('output.list','wb') as outputList_file:\r\n",
    "            pickle.dump(outputList,outputList_file)\r\n",
    "\r\n",
    "        print(\"Complete! (2/2)\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "daShape = None\n",
    "\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        #print(type(inputList+outputList))\n",
    "        tokenizer.fit_on_texts(inputList)\n",
    "        #print(inputList[0:10])\n",
    "        inputList = tokenizer.texts_to_sequences(inputList)\n",
    "        outputList = tokenizer.texts_to_sequences(outputList)\n",
    "\n",
    "        #print(inputList)\n",
    "        \n",
    "        inputList = pad_sequences(inputList)\n",
    "        outputList = pad_sequences(outputList)\n",
    "\n",
    "\n",
    "        print(inputList.shape)\n",
    "        print(outputList.shape)\n",
    "        if inputList.shape[1] > outputList.shape[1]:\n",
    "            daMaxLen = inputList.shape[1]\n",
    "        else:\n",
    "            daMaxLen = outputList.shape[1]\n",
    "        print(daMaxLen)\n",
    "\n",
    "        inputList = pad_sequences(inputList, maxlen = daMaxLen)\n",
    "        outputList = pad_sequences(outputList, maxlen = daMaxLen)\n",
    "\n",
    "        daShape = outputList.shape\n",
    "\n",
    "        \n",
    "\n",
    "        inputList = np.expand_dims(inputList,axis=0)\n",
    "        outputList = np.expand_dims(outputList,axis=0)\n",
    "\n",
    "        #outputList = to_categorical(outputList)\n",
    "\n",
    "\n",
    "        print(inputList[0:10])\n",
    "        print(inputList.shape)\n",
    "        inputList,outputList = inputList[0],outputList[0]\n",
    "        print(inputList.shape)\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "        print(\"Complete!\")\n",
    "print(\"sus\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('input.list','rb') as inputList_file:\r\n",
    "    with open('output.list','rb') as outputList_file:\r\n",
    "        outputList = pickle.load(outputList_file)\r\n",
    "        inputList = pickle.load(inputList_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, TimeDistributed\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "from tensorflow.keras.optimizers import RMSprop\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "x = 50\n",
    "\n",
    "eInput = Input(shape = (daMaxLen,))\n",
    "vocabSize = len(tokenizer.word_index) #gets the vocab size from tokenizer, duh\n",
    "\n",
    "\n",
    "EmbeddingLayer = Embedding(vocabSize,output_dim=x,input_length=daMaxLen,trainable=True)#vocabsize, number of output dimensions(try 8), inputlength(the padded length of the things)\n",
    "\n",
    "eEmbed = EmbeddingLayer(eInput)\n",
    "\n",
    "\n",
    "EncoderLSTM = LSTM(400,return_sequences=True,return_state=True)#number of neurons, input shape arg1 is number of timesteps, arg2 is dimensions of words/embeddings\n",
    "encoder_output,encoder_h,encoder_c = EncoderLSTM(eEmbed)\n",
    "# encoder_output,encoder_h,encoder_c = EncoderLSTM(EmbeddingOutput)\n",
    "\n",
    "dInput = Input(shape = (daMaxLen,))\n",
    "\n",
    "dEmbed = EmbeddingLayer(dInput)\n",
    "\n",
    "DecoderLSTM = LSTM(400,return_sequences=True,return_state=True)\n",
    "decoder_output, _, _ = DecoderLSTM(dEmbed,initial_state=[encoder_h,encoder_c])\n",
    "print(decoder_output.shape)\n",
    "\n",
    "dense = Dense(vocabSize,activation = 'softmax')\n",
    "\n",
    "finalOutput = dense(decoder_output)\n",
    "#Decode_Embedding = Dense(daMaxLen,activation='softmax')\n",
    "#finalOutput = Decode_Embedding(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "#Decode_Embedding2 = Dense(vocabSize,input_dim=dense_output.shape, activation=\"softmax\")\n",
    "#finalOutput = Decode_Embedding2(dense_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #Inverse tokenization\n",
    "# reverse_word_map = tokenizer.word_index\n",
    "# #print(reverse_word_map.keys())\n",
    "# reverse_word_map = {v: k for k, v in reverse_word_map.items()}\n",
    "# reverse_word_map[0] = \"\"\n",
    "\n",
    "#Model\n",
    "\n",
    "inputList = np.array(inputList)\n",
    "inputList = inputList[:10]\n",
    "outputList = np.array(outputList)\n",
    "outputList = outputList[:10]\n",
    "print(inputList.shape,outputList.shape)\n",
    "\n",
    "batch_size,epochs=256,10\n",
    "                        #inputs       #outputs\n",
    "training_model = Model([eInput,dInput], finalOutput)#Compiling\n",
    "training_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])#Training\n",
    "print(training_model.summary())\n",
    "checkpoint = [ModelCheckpoint(filepath='model3.hdf5')]\n",
    "finalOutputList = to_categorical(outputList,vocabSize)\n",
    "print(finalOutputList.shape)\n",
    "training_model.fit([inputList,outputList],finalOutputList, epochs = epochs,callbacks=checkpoint)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stats"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoderStateModel = Model([eInput],[encoder_h,encoder_c])\n",
    "decoder_input_h = Input(shape=(400,))\n",
    "decoder_input_c = Input(shape=(400,))\n",
    "\n",
    "decoder_outputs, decoder_h, decoder_c = DecoderLSTM(dEmbed, initial_state= [decoder_input_h,decoder_input_c])\n",
    "\n",
    "\n",
    "modelInput = \"Hello!\"\n",
    "\n",
    "removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\n",
    "removeCaps = lambda x: x.lower()\n",
    "\n",
    "print(modelInput)\n",
    "\n",
    "modelInput = removePunctuation(removeCaps(modelInput))\n",
    "modelInput = tokenizer.texts_to_sequences([modelInput])\n",
    "modelInput = pad_sequences(modelInput, maxlen = daMaxLen)\n",
    "modelInput = np.expand_dims(modelInput,axis=0)\n",
    "\n",
    "print(tokenizer.word_index[\"hello\"])\n",
    "\n",
    "\n",
    "#outputList = to_categorical(outputList)\n",
    "\n",
    "#modelInput = modelInput.astype(np.uint8)\n",
    "print(modelInput)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('input.list','rb') as inputList:\r\n",
    "    with open('output.list','rb') as outputList:\r\n",
    "        inputList = pickle.load(inputList)\r\n",
    "        outputList = pickle.load(outputList)\r\n",
    "        inputWordCounter = 0\r\n",
    "        inputCharCounter = 0\r\n",
    "        outputWordCounter = 0\r\n",
    "        outputCharCounter = 0\r\n",
    "        for message in range(len(inputList)):\r\n",
    "            for char in inputList[message]:\r\n",
    "                if char == \" \":\r\n",
    "                    inputWordCounter += 1\r\n",
    "                inputCharCounter += 1\r\n",
    "        for message in range(len(inputList)):\r\n",
    "            for char in outputList[message]:\r\n",
    "                if char == \" \":\r\n",
    "                    outputWordCounter += 1\r\n",
    "                outputCharCounter += 1\r\n",
    "        print(\"Count of Input Words: \" + str(inputWordCounter))\r\n",
    "        print(\"Count of Input Characters: \" + str(inputCharCounter))\r\n",
    "        print(\"Count of Output Words: \" + str(outputWordCounter))\r\n",
    "        print(\"Count of Output Characters: \" + str(outputCharCounter))\r\n",
    "print(\"sus\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}