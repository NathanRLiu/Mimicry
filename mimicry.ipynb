{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports 2021.2.576440691\n",
    "import numpy as np\n",
    "print(\"numpy imported\")\n",
    "import json\n",
    "print(\"json imported\")\n",
    "import time\n",
    "print(\"time imported\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"tensorflow imported\") \n",
    "from keras import Sequential\n",
    "print(\"keras imported\")\n",
    "import config\n",
    "channelID,serverID,targetID = config.getScope()\n",
    "print(\"config imported\")\n",
    "import pickle\n",
    "print(\"pickle imported\")\n",
    "import random\n",
    "print(\"random imported\")\n",
    "import string\n",
    "print(\"string imported\")\n",
    "from scipy.sparse import csc_matrix\n",
    "print(\"scipy imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the section below to process your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(filename):\n",
    "    try:\n",
    "        f = open(filename, \"rb\")\n",
    "        f.close()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "if not exists(\"message.list\"):\n",
    "    channelID,serverID,targetID = config.getScope()\n",
    "    daTestDoc = open(\"TestDoc\",\"r\",encoding=\"utf-8\")\n",
    "    daMessageList = eval(daTestDoc.read())\n",
    "    with open('message.list','wb') as messageList_file:\n",
    "        pickle.dump(daMessageList,messageList_file)\n",
    "        print(\":flushed:\")\n",
    "print('not flushed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputList,outputList = [],[]\n",
    "with open(\"message.list\",'rb') as daMessageList:\n",
    "    channelID,serverID,targetID = config.getScope()\n",
    "    daMessageList = pickle.load(daMessageList)\n",
    "    for i in range(len(daMessageList)):\n",
    "        if i != len(daMessageList)+1:\n",
    "            message = daMessageList[i]\n",
    "            if message[\"author\"][\"id\"] == str(targetID):\n",
    "                #if i != 0:\n",
    "                if True:\n",
    "                    previousMessage = daMessageList[i-1]\n",
    "                    if previousMessage[\"author\"][\"id\"] == str(targetID):\n",
    "                        if len(outputList) != 0:    \n",
    "                            outputList[len(outputList)-1] += \"   \" + message[\"content\"]\n",
    "                    else:\n",
    "                        if type(message.get(\"referenced_message\"))!=dict:\n",
    "                            outputList.append(message[\"content\"])\n",
    "                            message = daMessageList[i+1]\n",
    "                            inputList.append(message[\"content\"])\n",
    "                        else:\n",
    "                            inputList.append(message[\"referenced_message\"][\"content\"])\n",
    "                            outputList.append(message[\"content\"])\n",
    "\n",
    "        myIndice = random.randint(0,len(inputList))\n",
    "\n",
    "    # for message in range(len(inputList)):\n",
    "    # \tinputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "    with open('input.list','wb') as inputList_file:\n",
    "        pickle.dump(inputList,inputList_file)\n",
    "    with open('output.list','wb') as outputList_file:\n",
    "        pickle.dump(outputList,outputList_file)\n",
    "    print(\"Complete! (1/2)\")\n",
    "print(inputList[random.randint(0,1000)])\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "        \n",
    "        removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\n",
    "        removeCaps = lambda x: x.lower()\n",
    "\n",
    "        for message in range(len(inputList)):\n",
    "            inputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "            outputList[message] = removePunctuation(removeCaps(outputList[message]))\n",
    "            inputList[message] = \"<bos> \" + inputList[message] + \" <eos>\"\n",
    "            outputList[message] = \"<bos> \" + outputList[message] + \" <eos>\"\n",
    "        #inputList = np.array(inputList)\n",
    "        #outputList = np.array(outputList)\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "daShape = None\n",
    "\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "        vocabList = {}\n",
    "        vocabDict = []#i am good at naming things C:\n",
    "        for x in inputList:\n",
    "            wordList = x.split()\n",
    "            for word in wordList:\n",
    "                if word in vocabList:\n",
    "                    vocabList[word] += 1\n",
    "                else:\n",
    "                    vocabList[word] = 1\n",
    "        for x in outputList:\n",
    "            wordList = x.split()\n",
    "            for word in wordList:\n",
    "                if word in vocabList:\n",
    "                    vocabList[word] += 1\n",
    "                else:\n",
    "                    vocabList[word] = 1\n",
    "        for word in vocabList.keys():\n",
    "            if vocabList[word] > 2:\n",
    "                vocabDict.append(word)\n",
    "        tokenizer = Tokenizer(oov_token=2)\n",
    "        #print(type(inputList+outputList))\n",
    "        tokenizer.fit_on_texts(vocabDict)\n",
    "        #print(inputList[0:10])\n",
    "        inputList = tokenizer.texts_to_sequences(inputList)\n",
    "        outputList = tokenizer.texts_to_sequences(outputList)\n",
    "\n",
    "        #print(inputList)\n",
    "        \n",
    "        inputList = pad_sequences(inputList,padding='post')\n",
    "        outputList = pad_sequences(outputList,padding='post')\n",
    "\n",
    "\n",
    "        print(inputList.shape)\n",
    "        print(outputList.shape)\n",
    "        if inputList.shape[1] > outputList.shape[1]:\n",
    "            daMaxLen = inputList.shape[1]\n",
    "        else:\n",
    "            daMaxLen = outputList.shape[1]\n",
    "        print(daMaxLen)\n",
    "\n",
    "\n",
    "        print(inputList[0])\n",
    "        inputList = pad_sequences(inputList, maxlen = daMaxLen,padding='post')\n",
    "        outputList = pad_sequences(outputList, maxlen = daMaxLen,padding='post')\n",
    "\n",
    "        print(inputList[0])\n",
    "\n",
    "        daShape = outputList.shape\n",
    "\n",
    "        \n",
    "\n",
    "        inputList = np.expand_dims(inputList,axis=0)\n",
    "        outputList = np.expand_dims(outputList,axis=0)\n",
    "\n",
    "        #outputList = to_categorical(outputList)\n",
    "\n",
    "\n",
    "        #print(inputList[0])\n",
    "        #print(inputList.shape)\n",
    "        inputList,outputList = inputList[0],outputList[0]\n",
    "        #print(inputList.shape)\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "        print(\"Complete!\")\n",
    "print(\"sus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "l = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding,LSTM\n",
    "x = 8\n",
    "\n",
    "eInput = Input(shape = (daMaxLen,),name=\"encoder_input\")\n",
    "vocabSize = len(tokenizer.word_index) + 1 #gets the vocab size from tokenizer, duh\n",
    "\n",
    "print(\"maxLen is \" + str(daMaxLen))\n",
    "\n",
    "EmbeddingLayer = Embedding(vocabSize,output_dim=x,input_length=daMaxLen,trainable=True,name=\"Embedding\")#vocabsize, number of output dimensions(try 8), inputlength(the padded length of the things)\n",
    "\n",
    "eEmbed = EmbeddingLayer(eInput)\n",
    "\n",
    "\n",
    "EncoderLSTM = LSTM(64,return_sequences=True,return_state=True,name=\"Encoder_LSTM\")#number of neurons, input shape arg1 is number of timesteps, arg2 is dimensions of words/embeddings\n",
    "_,encoder_h,encoder_c = EncoderLSTM(eEmbed)\n",
    "\n",
    "dInput = Input(shape = (daMaxLen,),name=\"decoder_input\")\n",
    "\n",
    "dEmbed = EmbeddingLayer(dInput)\n",
    "\n",
    "print(dEmbed.shape)\n",
    "\n",
    "DecoderLSTM = LSTM(64,return_sequences=True,return_state=True,name=\"Decoder_LSTM\")\n",
    "decoder_output, _, _ = DecoderLSTM(dEmbed,initial_state=[encoder_h,encoder_c])\n",
    "print(decoder_output.shape)\n",
    "\n",
    "dense = Dense(vocabSize,activation = 'softmax',name=\"Dense\")\n",
    "print(vocabSize)\n",
    "finalOutput = dense(decoder_output)\n",
    "print(finalOutput.shape)\n",
    "#Decode_Embedding = Dense(daMaxLen,activation='softmax')\n",
    "#finalOutput = Decode_Embedding(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "#Decode_Embedding2 = Dense(vocabSize,input_dim=dense_output.shape, activation=\"softmax\")\n",
    "#finalOutput = Decode_Embedding2(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is an error so that I can hit run all and Jupyter won't run the next cell :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocabList))\n",
    "daModel.summary()\n",
    "plot_model(daModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "# #Inverse tokenization\n",
    "# reverse_word_map = tokenizer.word_index\n",
    "# #print(reverse_word_map.keys())\n",
    "# reverse_word_map = {v: k for k, v in reverse_word_map.items()}\n",
    "# reverse_word_map[0] = \"\"\n",
    "\n",
    "#Model\n",
    "\n",
    "\n",
    "                        #inputs       #outputs\n",
    "training_model = Model([eInput,dInput], finalOutput)#Compiling\n",
    "training_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])#Training\n",
    "print(training_model.summary())\n",
    "\n",
    "csv_logger = CSVLogger(\"model16_history_lconsole.logog.csv\", append=True)\n",
    "\n",
    "\n",
    "checkpoint = [csv_logger]\n",
    "checkpoint = [ModelCheckpoint(filepath='model1'),csv_logger]\n",
    "\n",
    "def generator(features,labels,batch_size):#this is necessary to avoid memory overload\n",
    "    batch_features = [np.zeros((batch_size,daMaxLen)),np.zeros((batch_size,daMaxLen))]\n",
    "    batch_labels = np.zeros((batch_size,daMaxLen,vocabSize))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            index = random.randint(0,len(labels)-1)\n",
    "            batch_features[0][i] = features[index]\n",
    "            batch_features[1][i] = features[index]\n",
    "            labelsPointer = labels[index]\n",
    "            # labelsPointer = batch_features[1][i][1:]\n",
    "            # labelsPointer = np.concatenate((labelsPointer,np.zeros((1,))),axis=0)\n",
    "            batch_labels[i] = to_categorical(labelsPointer,vocabSize)\n",
    "        yield batch_features,batch_labels\n",
    "\n",
    "#maybe have to make input/output into nparray\n",
    "epochs,batch_size = 10000,64\n",
    "training_model.fit(generator(inputList,outputList,batch_size), steps_per_epoch = len(inputList), epochs = epochs,callbacks=checkpoint,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "daModel = keras.models.load_model(\"model1\")\n",
    "\n",
    "#Embedding\n",
    "EmbeddingLayer.set_weights(daModel.get_layer(\"Embedding\").get_weights())\n",
    "#EncoderLSTM\n",
    "EncoderLSTM.set_weights(daModel.get_layer(\"Encoder_LSTM\").get_weights())\n",
    "#DecoderLSTM\n",
    "DecoderLSTM.set_weights(daModel.get_layer(\"Decoder_LSTM\").get_weights())\n",
    "#Dense\n",
    "dense.set_weights(daModel.get_layer(\"Dense\").get_weights())\n",
    "\n",
    "encoderStateModel = Model([eInput],[encoder_h,encoder_c])\n",
    "decoder_input_h = Input(shape=(64,))\n",
    "decoder_input_c = Input(shape=(64,))\n",
    "\n",
    "dn = EmbeddingLayer.get_weights()[0]\n",
    "\n",
    "decoder_outputs, decoder_h, decoder_c = DecoderLSTM(dEmbed, initial_state= [decoder_input_h,decoder_input_c])\n",
    "\n",
    "decoderOutputModel = Model([dInput]+[decoder_input_h,decoder_input_c], [decoder_outputs]+[decoder_h,decoder_c])\n",
    "\n",
    "modelInput = \"<bos> have you done deca <eos>\"\n",
    "\n",
    "removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\n",
    "removeCaps = lambda x: x.lower()\n",
    "\n",
    "\n",
    "modelInput = removePunctuation(removeCaps(modelInput))\n",
    "modelInput = tokenizer.texts_to_sequences([modelInput])\n",
    "modelInput = pad_sequences(modelInput, maxlen = daMaxLen,padding='post')\n",
    "modelInput = np.expand_dims(modelInput,axis=0)\n",
    "\n",
    "#modelInput = inputList[1]\n",
    "#modelInput = np.expand_dims(modelInput,axis=0)\n",
    "#modelInput = np.expand_dims(modelInput,axis=0)\n",
    "\n",
    "\n",
    "#outputList = to_categorical(outputList)\n",
    "outputArray = np.zeros((1,1))\n",
    "outputArray[0,0] = tokenizer.word_index[\"bos\"]\n",
    "\n",
    "#modelInput = modelInput.astype(np.uint8)\n",
    "\n",
    "print(modelInput[0])\n",
    "p = encoderStateModel.predict(modelInput[0])\n",
    "\n",
    "\n",
    "inv_wordIndex = {v: k for k, v in tokenizer.word_index.items()}\n",
    "inv_wordIndex[0] = \"fil\"\n",
    "mostLikely = \"\"\n",
    "outputString = \"\"\n",
    "counter = 0\n",
    "print(inv_wordIndex[1])\n",
    "\n",
    "#returnString = ''\n",
    "#for i in modelInput[0][0]:\n",
    "#    returnString += inv_wordIndex[i] + ' '\n",
    "#print(returnString)\n",
    "#returnString2 = ''\n",
    "#for i in outputList[2283]:\n",
    "#    returnString2 += inv_wordIndex[i] + ' '\n",
    "#print(returnString2)\n",
    "#def beamSearch(k,):\n",
    "while not mostLikely in [\"eos\"]:\n",
    "    if counter == 20:\n",
    "        break\n",
    "    dOutput, d_h, d_c = decoderOutputModel.predict([outputArray]+p)\n",
    "    wordProbabilities = dense(dOutput)\n",
    "    print(\"sex\\n\")\n",
    "    print(wordProbabilities[0][0])\n",
    "    mostLikely = np.argmax(wordProbabilities[0][0])\n",
    "    x = (np.argpartition(wordProbabilities[0][0], -4)[-4:])\n",
    "    if [mostLikely] == outputArray and counter != 0:\n",
    "        mostLikely = x[0]\n",
    "    for y in x:\n",
    "        print(str(inv_wordIndex[y]) + \" - \" + str(wordProbabilities[0][0][y]))\n",
    "    outputArray = np.zeros((1, 1))\n",
    "    outputArray[0, 0] = mostLikely\n",
    "    print(outputArray)\n",
    "    mostLikely = inv_wordIndex[mostLikely]\n",
    "    print(x)\n",
    "    \n",
    "    outputString += \" \"+str(mostLikely)+\" \"\n",
    "    p = [d_h,d_c]\n",
    "    print(outputString)\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.Tensor() output, a textual representation of an NP array containing the final output, contains a scalar probability for each possible next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daModel = keras.models.load_model(\"model3\")\n",
    "print(daModel.history.keys())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
