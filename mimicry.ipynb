{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "These are our imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "print(\"numpy imported\")\n",
    "import json\n",
    "print(\"json imported\")\n",
    "import time\n",
    "print(\"time imported\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"tensorflow imported\") \n",
    "from keras import Sequential\n",
    "print(\"keras imported\")\n",
    "import config\n",
    "channelID,serverID,targetID = config.getScope()\n",
    "print(\"config imported\")\n",
    "import pickle\n",
    "print(\"pickle imported\")\n",
    "import random\n",
    "print(\"random imported\")\n",
    "import string\n",
    "print(\"string imported\")\n",
    "from discAutoMsg import discord_automessage\n",
    "print(\"Discord Auto Message library imported\")\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numpy imported\n",
      "json imported\n",
      "time imported\n",
      "tensorflow imported\n",
      "keras imported\n",
      "config imported\n",
      "pickle imported\n",
      "random imported\n",
      "string imported\n",
      "{\"token\": \"MzA4MzAwOTIzMzcxOTEzMjE4.YNP-0Q.H7_KbCQ6eWdNwih8xbPi1ioCuzU\", \"user_settings\": {\"locale\": \"en-US\", \"theme\": \"dark\"}}\n",
      "Discord Auto Message library imported\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Run the section below to process your data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ":flushed:\n"
     ]
    }
   ],
   "source": [
    "channelID,serverID,targetID = config.getScope()\n",
    "daTestDoc = open(\"TestDoc\",\"r\",encoding=\"utf-8\")\n",
    "daMessageList = eval(daTestDoc.read())\n",
    "with open('message.list','wb') as messageList_file:\n",
    "    pickle.dump(daMessageList,messageList_file)\n",
    "    print(\":flushed:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete! (1/2)\nComplete! (2/2)\n"
     ]
    }
   ],
   "source": [
    "inputList,outputList = [],[]\n",
    "with open(\"message.list\",'rb') as daMessageList:\n",
    "    channelID,serverID,targetID = config.getScope()\n",
    "    daMessageList = pickle.load(daMessageList)\n",
    "    for i in range(len(daMessageList)):\n",
    "        if i != len(daMessageList)+1:\n",
    "            message = daMessageList[i]\n",
    "            if message[\"author\"][\"id\"] == str(targetID):\n",
    "                #if i != 0:\n",
    "                if True:\n",
    "                    previousMessage = daMessageList[i-1]\n",
    "                    if previousMessage[\"author\"][\"id\"] == str(targetID):\n",
    "                        if len(outputList) != 0:    \n",
    "                            outputList[len(outputList)-1] += \"   \" + message[\"content\"]\n",
    "                    else:\n",
    "                        if type(message.get(\"referenced_message\"))!=dict:\n",
    "                            outputList.append(message[\"content\"])\n",
    "                            message = daMessageList[i+1]\n",
    "                            inputList.append(message[\"content\"])\n",
    "                        else:\n",
    "                            inputList.append(message[\"referenced_message\"][\"content\"])\n",
    "                            outputList.append(message[\"content\"])\n",
    "\n",
    "        myIndice = random.randint(0,len(inputList))\n",
    "\n",
    "    # for message in range(len(inputList)):\n",
    "    # \tinputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "    with open('input.list','wb') as inputList_file:\n",
    "        pickle.dump(inputList,inputList_file)\n",
    "    with open('output.list','wb') as outputList_file:\n",
    "        pickle.dump(outputList,outputList_file)\n",
    "    print(\"Complete! (1/2)\")\n",
    "\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "        \n",
    "        removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\n",
    "        removeCaps = lambda x: x.lower()\n",
    "\n",
    "        for message in range(len(inputList)):\n",
    "            inputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "            outputList[message] = removePunctuation(removeCaps(outputList[message]))\n",
    "        #inputList = np.array(inputList)\n",
    "        #outputList = np.array(outputList)\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "\n",
    "        print(\"Complete! (2/2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   0    0    0 ...    8   39  287]\n [   0    0    0 ...    7 1100  287]\n [   0    0    0 ...   13 1101   47]\n ...\n [   0    0    0 ...    8  393  270]\n [   0    0    0 ...   24 2788  126]\n [   0    0    0 ...    0   84   18]]\n"
     ]
    }
   ],
   "source": [
    "with open('input.list','rb') as inputList_file:\n",
    "    inputList = pickle.load(inputList_file)\n",
    "    daRandInt = random.randint(0,len(inputList)-1)\n",
    "    print(inputList[daRandInt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-36f1b281ba3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print(type(inputList+outputList))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(inputList[0:10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0minputList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                 seq = text_to_word_sequence(text,\n\u001b[0m\u001b[0;32m    223\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "daShape = None\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "\n",
    "        daMaxLen = 0\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        #print(type(inputList+outputList))\n",
    "        tokenizer.fit_on_texts(inputList)\n",
    "        #print(inputList[0:10])\n",
    "        inputList = tokenizer.texts_to_sequences(inputList)\n",
    "        outputList = tokenizer.texts_to_sequences(outputList)\n",
    "\n",
    "        #print(inputList)\n",
    "\n",
    "        inputList = pad_sequences(inputList)\n",
    "        outputList = pad_sequences(outputList)\n",
    "\n",
    "        if inputList.shape[1] < outputList.shape[1]:\n",
    "            daMaxLen = inputList.shape[1]\n",
    "        else:\n",
    "            daMaxLen = outputList.shape[1]\n",
    "\n",
    "        inputList = pad_sequences(inputList, maxlen = daMaxLen)\n",
    "        outputList = pad_sequences(outputList, maxlen = daMaxLen)\n",
    "\n",
    "        daShape = outputList.shape\n",
    "\n",
    "        inputList = np.expand_dims(inputList,axis=0)\n",
    "        outputList = np.expand_dims(outputList,axis=0)\n",
    "\n",
    "        inputList = to_categorical(inputList)\n",
    "        outputList = to_categorical(outputList)\n",
    "\n",
    "        inputList = inputList.astype(np.uint8)\n",
    "        outputList = outputList.astype(np.uint8)\n",
    "\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "        print(\"Complete!\")"
   ]
  },
  {
   "source": [
    "** This is a test to make sure the filters run correctly. **"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "!\n",
      "<ipython-input-59-1b838eb49a3f>:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if letter in inputList[message]:\n",
      "<ipython-input-59-1b838eb49a3f>:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if letter in outputList[message]:\n"
     ]
    }
   ],
   "source": [
    "with open('input.list','rb') as inputList:\n",
    "    with open('output.list','rb') as outputList:\n",
    "        inputList = pickle.load(inputList)\n",
    "        outputList = pickle.load(outputList)\n",
    "        for message in range(len(inputList)):\n",
    "            for letter in (\"abcdefghijklmnopqrstuvwxyz\".upper()+string.punctuation):\n",
    "                if letter in inputList[message]:\n",
    "                    print(inputList[message])\n",
    "                if letter in outputList[message]:\n",
    "                    print(outputList[message])\n",
    "print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ":pray:\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "tf.keras.layers.Input(shape=(daShape))\n",
    "model.add(layers.LSTM(2, activation=\"tanh\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"softmax\", name=\"layer2\"))\n",
    "model.add(layers.Dense(4, name=\"layer3\"))\n",
    "print(\":pray:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "rmsprop = RMSprop(learning_rate=0.0001)\n",
    "with open('input.list','rb') as inputList:\n",
    "    with open('output.list','rb') as outputList:\n",
    "        inputList = pickle.load(inputList)\n",
    "        outputList = pickle.load(outputList)\n",
    "        print(type(inputList),type(outputList))\n",
    "        model.compile(optimizer = rmsprop, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        #View model info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(inputList,outputList, epochs = 1000, batch_size = 40)"
   ]
  },
  {
   "source": [
    "Stats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_25\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlayer1 (LSTM)                multiple                  664       \n_________________________________________________________________\nlayer2 (Dense)               multiple                  0 (unused)\n_________________________________________________________________\nlayer3 (Dense)               multiple                  0 (unused)\n=================================================================\nTotal params: 664\nTrainable params: 664\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count of Input Words: 15369\nCount of Input Characters: 104750\nCount of Output Words: 11887\nCount of Output Characters: 83382\n"
     ]
    }
   ],
   "source": [
    "with open('input.list','rb') as inputList:\n",
    "    with open('output.list','rb') as outputList:\n",
    "        inputList = pickle.load(inputList)\n",
    "        outputList = pickle.load(outputList)\n",
    "        inputWordCounter = 0\n",
    "        inputCharCounter = 0\n",
    "        outputWordCounter = 0\n",
    "        outputCharCounter = 0\n",
    "        for message in range(len(inputList)):\n",
    "            for char in inputList[message]:\n",
    "                if char == \" \":\n",
    "                    inputWordCounter += 1\n",
    "                inputCharCounter += 1\n",
    "        for message in range(len(inputList)):\n",
    "            for char in outputList[message]:\n",
    "                if char == \" \":\n",
    "                    outputWordCounter += 1\n",
    "                outputCharCounter += 1\n",
    "        print(\"Count of Input Words: \" + str(inputWordCounter))\n",
    "        print(\"Count of Input Characters: \" + str(inputCharCounter))\n",
    "        print(\"Count of Output Words: \" + str(outputWordCounter))\n",
    "        print(\"Count of Output Characters: \" + str(outputCharCounter))\n"
   ]
  }
 ]
}