{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39564bit1363f0ebf2474417a338a3c0899d36d4",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "These are our imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "print(\"numpy imported\")\n",
    "import json\n",
    "print(\"json imported\")\n",
    "import time\n",
    "print(\"time imported\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"tensorflow imported\") \n",
    "from keras import Sequential\n",
    "print(\"keras imported\")\n",
    "import config\n",
    "channelID,serverID,targetID = config.getScope()\n",
    "print(\"config imported\")\n",
    "import pickle\n",
    "print(\"pickle imported\")\n",
    "import random\n",
    "print(\"random imported\")\n",
    "import string\n",
    "print(\"string imported\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Run the section below to process your data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelID,serverID,targetID = config.getScope()\n",
    "daTestDoc = open(\"TestDoc\",\"r\",encoding=\"utf-8\")\n",
    "daMessageList = eval(daTestDoc.read())\n",
    "with open('message.list','wb') as messageList_file:\n",
    "    pickle.dump(daMessageList,messageList_file)\n",
    "    print(\":flushed:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputList,outputList = [],[]\n",
    "with open(\"message.list\",'rb') as daMessageList:\n",
    "    channelID,serverID,targetID = config.getScope()\n",
    "    daMessageList = pickle.load(daMessageList)\n",
    "    for i in range(len(daMessageList)):\n",
    "        if i != len(daMessageList)+1:\n",
    "            message = daMessageList[i]\n",
    "            if message[\"author\"][\"id\"] == str(targetID):\n",
    "                #if i != 0:\n",
    "                if True:\n",
    "                    previousMessage = daMessageList[i-1]\n",
    "                    if previousMessage[\"author\"][\"id\"] == str(targetID):\n",
    "                        if len(outputList) != 0:    \n",
    "                            outputList[len(outputList)-1] += \"   \" + message[\"content\"]\n",
    "                    else:\n",
    "                        if type(message.get(\"referenced_message\"))!=dict:\n",
    "                            outputList.append(message[\"content\"])\n",
    "                            message = daMessageList[i+1]\n",
    "                            inputList.append(message[\"content\"])\n",
    "                        else:\n",
    "                            inputList.append(message[\"referenced_message\"][\"content\"])\n",
    "                            outputList.append(message[\"content\"])\n",
    "\n",
    "        myIndice = random.randint(0,len(inputList))\n",
    "\n",
    "    # for message in range(len(inputList)):\n",
    "    # \tinputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "    with open('input.list','wb') as inputList_file:\n",
    "        pickle.dump(inputList,inputList_file)\n",
    "    with open('output.list','wb') as outputList_file:\n",
    "        pickle.dump(outputList,outputList_file)\n",
    "    print(\"Complete! (1/2)\")\n",
    "\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "        \n",
    "        removePunctuation = lambda x: ''.join(char for char in x if not char in string.punctuation)\n",
    "        removeCaps = lambda x: x.lower()\n",
    "\n",
    "        for message in range(len(inputList)):\n",
    "            inputList[message] = removePunctuation(removeCaps(inputList[message]))\n",
    "            outputList[message] = removePunctuation(removeCaps(outputList[message]))\n",
    "        #inputList = np.array(inputList)\n",
    "        #outputList = np.array(outputList)\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "\n",
    "        print(\"Complete! (2/2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.list','rb') as inputList_file:\n",
    "    inputList = pickle.load(inputList_file)\n",
    "    daRandInt = random.randint(0,len(inputList)-1)\n",
    "    print(outputList[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del daMessageList\n",
    "del outputList\n",
    "del daTestDoc\n",
    "del inputList\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daShape = None\n",
    "\n",
    "\n",
    "with open('input.list','rb') as inputList_file:\n",
    "    with open('output.list','rb') as outputList_file:\n",
    "        outputList = pickle.load(outputList_file)\n",
    "        inputList = pickle.load(inputList_file)\n",
    "\n",
    "        #outputList = [\"hello there\",\"among us in real life\", \"hello impostor\"]\n",
    "        #inputList = [\"hey\",\"im so sus\",\"omg emergency meeting\"]\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        #print(type(inputList+outputList))\n",
    "        tokenizer.fit_on_texts(inputList)\n",
    "        #print(inputList[0:10])\n",
    "        inputList = tokenizer.texts_to_sequences(inputList)\n",
    "        outputList = tokenizer.texts_to_sequences(outputList)\n",
    "\n",
    "        #print(inputList)\n",
    "        \n",
    "        inputList = pad_sequences(inputList)\n",
    "        outputList = pad_sequences(outputList)\n",
    "\n",
    "\n",
    "        print(inputList.shape)\n",
    "        print(outputList.shape)\n",
    "        if inputList.shape[1] > outputList.shape[1]:\n",
    "            daMaxLen = inputList.shape[1]\n",
    "        else:\n",
    "            daMaxLen = outputList.shape[1]\n",
    "        print(daMaxLen)\n",
    "\n",
    "        inputList = pad_sequences(inputList, maxlen = daMaxLen)\n",
    "        outputList = pad_sequences(outputList, maxlen = daMaxLen)\n",
    "\n",
    "        daShape = outputList.shape\n",
    "\n",
    "        inputList = np.expand_dims(inputList,axis=0)\n",
    "        outputList = np.expand_dims(outputList,axis=0)\n",
    "\n",
    "        inputList = to_categorical(inputList)\n",
    "        outputList = to_categorical(outputList)\n",
    "\n",
    "        inputList = inputList.astype(np.uint8)\n",
    "        outputList = outputList.astype(np.uint8)\n",
    "\n",
    "        with open('input.list','wb') as inputList_file:\n",
    "            pickle.dump(inputList,inputList_file)\n",
    "        with open('output.list','wb') as outputList_file:\n",
    "            pickle.dump(outputList,outputList_file)\n",
    "        print(\"Complete!\")"
   ]
  },
  {
   "source": [
    "** This is a test to make sure the filters run correctly. **"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.list','rb') as inputList:\n",
    "    with open('output.list','rb') as outputList:\n",
    "        inputList = pickle.load(inputList)\n",
    "        outputList = pickle.load(outputList)\n",
    "        for message in range(len(inputList)):\n",
    "            for letter in (\"abcdefghijklmnopqrstuvwxyz\".upper()+string.punctuation):\n",
    "                if letter in inputList[message]:\n",
    "                    print(inputList[message])\n",
    "                if letter in outputList[message]:\n",
    "                    print(outputList[message])\n",
    "print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model config\n",
    "print(inputList.shape)\n",
    "print(outputList.shape)\n",
    "randomIndice = random.randint(0,len(inputList)-1)\n",
    "print(inputList[0][0][0].shape)\n",
    "\n",
    "dataSize = 2789 #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 256#The batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 600#Encoder\n",
    "num_encoder_tokens = dataSize\n",
    "num_decoder_tokens = dataSize\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]#Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer = rmsprop, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "#View model info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse tokenization\n",
    "reverse_word_map = tokenizer.word_index\n",
    "#print(reverse_word_map.keys())\n",
    "reverse_word_map = {v: k for k, v in reverse_word_map.items()}\n",
    "reverse_word_map[0] = \"\"\n",
    "\n",
    "#Model\n",
    "\n",
    "training_model = Model(wordList, wordList, decoder_outputs)#Compiling\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')#Training\n",
    "training_model.fit(inputList[0], outputList[0], batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "training_model.save('training_model.h5')"
   ]
  },
  {
   "source": [
    "Stats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.list','rb') as inputList:\n",
    "    with open('output.list','rb') as outputList:\n",
    "        inputList = pickle.load(inputList)\n",
    "        outputList = pickle.load(outputList)\n",
    "        inputWordCounter = 0\n",
    "        inputCharCounter = 0\n",
    "        outputWordCounter = 0\n",
    "        outputCharCounter = 0\n",
    "        for message in range(len(inputList)):\n",
    "            for char in inputList[message]:\n",
    "                if char == \" \":\n",
    "                    inputWordCounter += 1\n",
    "                inputCharCounter += 1\n",
    "        for message in range(len(inputList)):\n",
    "            for char in outputList[message]:\n",
    "                if char == \" \":\n",
    "                    outputWordCounter += 1\n",
    "                outputCharCounter += 1\n",
    "        print(\"Count of Input Words: \" + str(inputWordCounter))\n",
    "        print(\"Count of Input Characters: \" + str(inputCharCounter))\n",
    "        print(\"Count of Output Words: \" + str(outputWordCounter))\n",
    "        print(\"Count of Output Characters: \" + str(outputCharCounter))\n"
   ]
  }
 ]
}